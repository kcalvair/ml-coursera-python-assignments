{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Exercise 5:\n",
    "# Regularized Linear Regression and Bias vs Variance\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this exercise, you will implement regularized linear regression and use it to study models with different bias-variance properties. Before starting on the programming exercise, we strongly recommend watching the video lectures and completing the review questions for the associated topics.\n",
    "\n",
    "All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook. The assignment can be promptly submitted to the coursera grader directly from this notebook (code and instructions are included below).\n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using [`numpy`](http://www.numpy.org/) for all arrays and matrix operations, [`matplotlib`](https://matplotlib.org/) for plotting, and [`scipy`](https://docs.scipy.org/doc/scipy/reference/) for scientific and numerical computation functions and tools. You can find instructions on how to install required libraries in the README file in the [github repository](https://github.com/dibgerge/ml-coursera-python-assignments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for manipulating directory paths\n",
    "import os\n",
    "\n",
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Optimization module in scipy\n",
    "from scipy import optimize\n",
    "\n",
    "# will be used to load MATLAB mat datafile format\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# library written for this exercise providing additional functions for assignment submission, and others\n",
    "import utils\n",
    "\n",
    "# define the submission/grader object for this exercise\n",
    "grader = utils.Grader()\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission and Grading\n",
    "\n",
    "\n",
    "After completing each part of the assignment, be sure to submit your solutions to the grader. The following is a breakdown of how each part of this exercise is scored.\n",
    "\n",
    "\n",
    "| Section | Part                                             | Submitted Function                | Points |\n",
    "| :-      |:-                                                |:-                                 | :-:    |\n",
    "| 1       | [Regularized Linear Regression Cost Function](#section1)      | [`linearRegCostFunction`](#linearRegCostFunction) |  25    |\n",
    "| 2       | [Regularized Linear Regression Gradient](#section2)           | [`linearRegCostFunction`](#linearRegCostFunction) |25      |\n",
    "| 3       | [Learning Curve](#section3)                                   | [`learningCurve`](#func2)         | 20     |\n",
    "| 4       | [Polynomial Feature Mapping](#section4)                       | [`polyFeatures`](#polyFeatures)          | 10     |\n",
    "| 5       | [Cross Validation Curve](#section5)                           | [`validationCurve`](#validationCurve)       | 20     |\n",
    "|         | Total Points                                     |                                   |100     |\n",
    "\n",
    "\n",
    "You are allowed to submit your solutions multiple times, and we will take only the highest score into consideration.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "At the end of each section in this notebook, we have a cell which contains code for submitting the solutions thus far to the grader. Execute the cell to see your score up to the current section. For all your work to be submitted properly, you must execute those cells at least once.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "## 1 Regularized Linear Regression\n",
    "\n",
    "In the first half of the exercise, you will implement regularized linear regression to predict the amount of water flowing out of a dam using the change of water level in a reservoir. In the next half, you will go through some diagnostics of debugging learning algorithms and examine the effects of bias v.s.\n",
    "variance. \n",
    "\n",
    "### 1.1 Visualizing the dataset\n",
    "\n",
    "We will begin by visualizing the dataset containing historical records on the change in the water level, $x$, and the amount of water flowing out of the dam, $y$. This dataset is divided into three parts:\n",
    "\n",
    "- A **training** set that your model will learn on: `X`, `y`\n",
    "- A **cross validation** set for determining the regularization parameter: `Xval`, `yval`\n",
    "- A **test** set for evaluating performance. These are “unseen” examples which your model did not see during training: `Xtest`, `ytest`\n",
    "\n",
    "Run the next cell to plot the training data. In the following parts, you will implement linear regression and use that to fit a straight line to the data and plot learning curves. Following that, you will implement polynomial regression to find a better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 382.669453 262.19625\" width=\"382.669453pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-02-26T20:26:45.723901</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 262.19625 \r\nL 382.669453 262.19625 \r\nL 382.669453 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\nL 375.403125 7.2 \r\nL 40.603125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m623fd8d870\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"84.49209\" xlink:href=\"#m623fd8d870\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- −40 -->\r\n      <g transform=\"translate(73.939746 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.59375 35.5 \r\nL 73.1875 35.5 \r\nL 73.1875 27.203125 \r\nL 10.59375 27.203125 \r\nz\r\n\" id=\"DejaVuSans-8722\"/>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"155.645806\" xlink:href=\"#m623fd8d870\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- −20 -->\r\n      <g transform=\"translate(145.093462 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-8722\"/>\r\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"226.799521\" xlink:href=\"#m623fd8d870\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(223.618271 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"297.953237\" xlink:href=\"#m623fd8d870\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(291.590737 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.106953\" xlink:href=\"#m623fd8d870\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(362.744453 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_6\">\r\n     <!-- Change in water level (x) -->\r\n     <g transform=\"translate(144.948438 252.916562)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path id=\"DejaVuSans-32\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       <path d=\"M 4.203125 54.6875 \r\nL 13.1875 54.6875 \r\nL 24.421875 12.015625 \r\nL 35.59375 54.6875 \r\nL 46.1875 54.6875 \r\nL 57.421875 12.015625 \r\nL 68.609375 54.6875 \r\nL 77.59375 54.6875 \r\nL 63.28125 0 \r\nL 52.6875 0 \r\nL 40.921875 44.828125 \r\nL 29.109375 0 \r\nL 18.5 0 \r\nz\r\n\" id=\"DejaVuSans-119\"/>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n       <path d=\"M 31 75.875 \r\nQ 24.46875 64.65625 21.28125 53.65625 \r\nQ 18.109375 42.671875 18.109375 31.390625 \r\nQ 18.109375 20.125 21.3125 9.0625 \r\nQ 24.515625 -2 31 -13.1875 \r\nL 23.1875 -13.1875 \r\nQ 15.875 -1.703125 12.234375 9.375 \r\nQ 8.59375 20.453125 8.59375 31.390625 \r\nQ 8.59375 42.28125 12.203125 53.3125 \r\nQ 15.828125 64.359375 23.1875 75.875 \r\nz\r\n\" id=\"DejaVuSans-40\"/>\r\n       <path d=\"M 54.890625 54.6875 \r\nL 35.109375 28.078125 \r\nL 55.90625 0 \r\nL 45.3125 0 \r\nL 29.390625 21.484375 \r\nL 13.484375 0 \r\nL 2.875 0 \r\nL 24.125 28.609375 \r\nL 4.6875 54.6875 \r\nL 15.28125 54.6875 \r\nL 29.78125 35.203125 \r\nL 44.28125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-120\"/>\r\n       <path d=\"M 8.015625 75.875 \r\nL 15.828125 75.875 \r\nQ 23.140625 64.359375 26.78125 53.3125 \r\nQ 30.421875 42.28125 30.421875 31.390625 \r\nQ 30.421875 20.453125 26.78125 9.375 \r\nQ 23.140625 -1.703125 15.828125 -13.1875 \r\nL 8.015625 -13.1875 \r\nQ 14.5 -2 17.703125 9.0625 \r\nQ 20.90625 20.125 20.90625 31.390625 \r\nQ 20.90625 42.671875 17.703125 53.65625 \r\nQ 14.5 64.65625 8.015625 75.875 \r\nz\r\n\" id=\"DejaVuSans-41\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"133.203125\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"194.482422\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.861328\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"321.337891\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"382.861328\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"414.648438\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"442.431641\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"505.810547\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"537.597656\" xlink:href=\"#DejaVuSans-119\"/>\r\n      <use x=\"619.384766\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"680.664062\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"719.873047\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"781.396484\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"822.509766\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"854.296875\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"882.080078\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"943.603516\" xlink:href=\"#DejaVuSans-118\"/>\r\n      <use x=\"1002.783203\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"1064.306641\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"1092.089844\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"1123.876953\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"1162.890625\" xlink:href=\"#DejaVuSans-120\"/>\r\n      <use x=\"1222.070312\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_6\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"md59079a2bf\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"221.259176\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.240625 225.058395)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"193.546517\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(27.240625 197.345736)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"165.833858\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(20.878125 169.633076)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"138.121198\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(20.878125 141.920417)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"110.408539\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(20.878125 114.207758)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"82.69588\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(20.878125 86.495099)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"54.983221\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(20.878125 58.782439)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#md59079a2bf\" y=\"27.270561\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 35 -->\r\n      <g transform=\"translate(20.878125 31.06978)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_15\">\r\n     <!-- Water flowing out of the dam (y) -->\r\n     <g transform=\"translate(14.798438 197.448125)rotate(-90)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 3.328125 72.90625 \r\nL 13.28125 72.90625 \r\nL 28.609375 11.28125 \r\nL 43.890625 72.90625 \r\nL 54.984375 72.90625 \r\nL 70.3125 11.28125 \r\nL 85.59375 72.90625 \r\nL 95.609375 72.90625 \r\nL 77.296875 0 \r\nL 64.890625 0 \r\nL 49.515625 63.28125 \r\nL 33.984375 0 \r\nL 21.578125 0 \r\nz\r\n\" id=\"DejaVuSans-87\"/>\r\n       <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-87\"/>\r\n      <use x=\"92.501953\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"153.78125\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"192.990234\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"254.513672\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"295.626953\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"327.414062\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"362.619141\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"390.402344\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"451.583984\" xlink:href=\"#DejaVuSans-119\"/>\r\n      <use x=\"533.371094\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"561.154297\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"624.533203\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"688.009766\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"719.796875\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"780.978516\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"844.357422\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"883.566406\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"915.353516\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"976.535156\" xlink:href=\"#DejaVuSans-102\"/>\r\n      <use x=\"1011.740234\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"1043.527344\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"1082.736328\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"1146.115234\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"1207.638672\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"1239.425781\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"1302.902344\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"1364.181641\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"1461.59375\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"1493.380859\" xlink:href=\"#DejaVuSans-40\"/>\r\n      <use x=\"1532.394531\" xlink:href=\"#DejaVuSans-121\"/>\r\n      <use x=\"1591.574219\" xlink:href=\"#DejaVuSans-41\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <defs>\r\n     <path d=\"M 0 5 \r\nC 1.326016 5 2.597899 4.473168 3.535534 3.535534 \r\nC 4.473168 2.597899 5 1.326016 5 0 \r\nC 5 -1.326016 4.473168 -2.597899 3.535534 -3.535534 \r\nC 2.597899 -4.473168 1.326016 -5 0 -5 \r\nC -1.326016 -5 -2.597899 -4.473168 -3.535534 -3.535534 \r\nC -4.473168 -2.597899 -5 -1.326016 -5 0 \r\nC -5 1.326016 -4.473168 2.597899 -3.535534 3.535534 \r\nC -2.597899 4.473168 -1.326016 5 0 5 \r\nz\r\n\" id=\"m049baaf14c\" style=\"stroke:#000000;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pd24c86469c)\">\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"170.101543\" xlink:href=\"#m049baaf14c\" y=\"209.429692\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"123.082381\" xlink:href=\"#m049baaf14c\" y=\"214.756364\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"355.550564\" xlink:href=\"#m049baaf14c\" y=\"30.822719\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"360.184943\" xlink:href=\"#m049baaf14c\" y=\"17.083636\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"55.821307\" xlink:href=\"#m049baaf14c\" y=\"205.690398\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"194.988623\" xlink:href=\"#m049baaf14c\" y=\"209.503064\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"281.259838\" xlink:href=\"#m049baaf14c\" y=\"139.727046\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"103.325533\" xlink:href=\"#m049baaf14c\" y=\"206.769976\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"231.741696\" xlink:href=\"#m049baaf14c\" y=\"200.529156\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"68.89605\" xlink:href=\"#m049baaf14c\" y=\"200.576158\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"251.751358\" xlink:href=\"#m049baaf14c\" y=\"178.982634\"/>\r\n     <use style=\"fill:#ff0000;stroke:#000000;\" x=\"307.78223\" xlink:href=\"#m049baaf14c\" y=\"95.153118\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 40.603125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 375.403125 224.64 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 40.603125 224.64 \r\nL 375.403125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 40.603125 7.2 \r\nL 375.403125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd24c86469c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"40.603125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# Load from ex5data1.mat, where all variables will be store in a dictionary\n",
    "data = loadmat(os.path.join('Data', 'ex5data1.mat'))\n",
    "\n",
    "# Extract train, test, validation data from dictionary\n",
    "# and also convert y's form 2-D matrix (MATLAB format) to a numpy vector\n",
    "X, y = data['X'], data['y'][:, 0]\n",
    "Xtest, ytest = data['Xtest'], data['ytest'][:, 0]\n",
    "Xval, yval = data['Xval'], data['yval'][:, 0]\n",
    "\n",
    "# m = Number of examples\n",
    "m = y.size\n",
    "\n",
    "# Plot training data\n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Regularized linear regression cost function\n",
    "\n",
    "Recall that regularized linear regression has the following cost function:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\left( \\sum_{i=1}^m \\left( h_\\theta\\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right) + \\frac{\\lambda}{2m} \\left( \\sum_{j=1}^n \\theta_j^2 \\right)$$\n",
    "\n",
    "where $\\lambda$ is a regularization parameter which controls the degree of regularization (thus, help preventing overfitting). The regularization term puts a penalty on the overall cost J. As the magnitudes of the model parameters $\\theta_j$ increase, the penalty increases as well. Note that you should not regularize\n",
    "the $\\theta_0$ term.\n",
    "\n",
    "You should now complete the code in the function `linearRegCostFunction` in the next cell. Your task is to calculate the regularized linear regression cost function. If possible, try to vectorize your code and avoid writing loops.\n",
    "<a id=\"linearRegCostFunction\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, lambda_=0.0):\n",
    "    \"\"\"\n",
    "    Compute cost and gradient for regularized linear regression \n",
    "    with multiple variables. Computes the cost of using theta as\n",
    "    the parameter for linear regression to fit the data points in X and y. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The dataset. Matrix with shape (m x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    theta : array_like\n",
    "        The parameters for linear regression. A vector of shape (n+1,).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        The regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        The computed cost function. \n",
    "    \n",
    "    grad : array_like\n",
    "        The value of the cost function gradient w.r.t theta. \n",
    "        A vector of shape (n+1, ).\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Compute the cost and gradient of regularized linear regression for\n",
    "    a particular choice of theta.\n",
    "    You should set J to the cost and grad to the gradient.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.size # number of training examples\n",
    "\n",
    "    # You need to return the following variables correctly \n",
    "    J = 0\n",
    "    grad = np.zeros(theta.shape)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    h = X.dot(theta)\n",
    "    J = (1/(2*m))*np.sum((h-y)**2) + (lambda_/(2*m))*np.sum(theta[1:]**2)\n",
    "\n",
    "    grad = (1/m)*(np.dot(X.T, (h-y)))\n",
    "    grad[1:] = grad[1:] + (lambda_/m)*theta[1:]\n",
    "\n",
    "    # ============================================================\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished, the next cell will run your cost function using `theta` initialized at `[1, 1]`. You should expect to see an output of 303.993."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cost at theta = [1, 1]:\t   303.993192 \nThis value should be about 303.993192)\n\n"
     ]
    }
   ],
   "source": [
    "theta = np.array([1, 1])\n",
    "J, _ = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)\n",
    "\n",
    "print('Cost at theta = [1, 1]:\\t   %f ' % J)\n",
    "print('This value should be about 303.993192)\\n' % J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing a part of the exercise, you can submit your solutions for grading by first adding the function you modified to the submission object, and then sending your function to Coursera for grading. \n",
    "\n",
    "The submission script will prompt you for your login e-mail and submission token. You can obtain a submission token from the web page for the assignment. You are allowed to submit your solutions multiple times, and we will take only the highest score into consideration.\n",
    "\n",
    "*Execute the following cell to grade your solution to the first part of this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Submitting Solutions | Programming Exercise regularized-linear-regression-and-bias-variance\n",
      "\n",
      "                                  Part Name |     Score | Feedback\n",
      "                                  --------- |     ----- | --------\n",
      "Regularized Linear Regression Cost Function |  25 /  25 | Nice work!\n",
      "     Regularized Linear Regression Gradient |   0 /  25 | \n",
      "                             Learning Curve |   0 /  20 | \n",
      "                 Polynomial Feature Mapping |   0 /  10 | \n",
      "                           Validation Curve |   0 /  20 | \n",
      "                                  --------------------------------\n",
      "                                            |  25 / 100 |  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grader[1] = linearRegCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "### 1.3 Regularized linear regression gradient\n",
    "\n",
    "Correspondingly, the partial derivative of the cost function for regularized linear regression is defined as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left(x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} & \\qquad \\text{for } j = 0 \\\\\n",
    "& \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m} \\theta_j & \\qquad \\text{for } j \\ge 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the function [`linearRegCostFunction`](#linearRegCostFunction) above, add code to calculate the gradient, returning it in the variable `grad`. <font color='red'><b>Do not forget to re-execute the cell containing this function to update the function's definition.</b></font>\n",
    "\n",
    "\n",
    "When you are finished, use the next cell to  run your gradient function using theta initialized at `[1, 1]`. You should expect to see a gradient of `[-15.30, 598.250]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-0d74a491d0b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mJ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinearRegCostFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Gradient at theta = [1, 1]:  [{:.6f}, {:.6f}] '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' (this value should be about [-15.303016, 598.250744])\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-61-d082fc7bc56a>\u001b[0m in \u001b[0;36mlinearRegCostFunction\u001b[1;34m(X, y, theta, lambda_)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;31m# ============================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "theta = np.array([1, 1])\n",
    "J, grad = linearRegCostFunction(np.concatenate([np.ones((m, 1)), X], axis=1), y, theta, 1)\n",
    "\n",
    "print('Gradient at theta = [1, 1]:  [{:.6f}, {:.6f}] '.format(*grad))\n",
    "print(' (this value should be about [-15.303016, 598.250744])\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nSubmitting Solutions | Programming Exercise regularized-linear-regression-and-bias-variance\n\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,) (2,) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-c3c62772595c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgrader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinearRegCostFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrade\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\eskim\\Documents\\GitHub\\ml-coursera-python-assignments\\submission.py\u001b[0m in \u001b[0;36mgrade\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Evaluate the different parts of exercise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mpart_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mparts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpart_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msprintf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%0.5f '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\eskim\\Documents\\GitHub\\ml-coursera-python-assignments\\Exercise5\\utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mpart_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mpart_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-bbe97502d7c2>\u001b[0m in \u001b[0;36mlinearRegCostFunction\u001b[1;34m(X, y, theta, lambda_)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,) (2,) "
     ]
    }
   ],
   "source": [
    "grader[2] = linearRegCostFunction\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting linear regression\n",
    "\n",
    "Once your cost function and gradient are working correctly, the next cell will run the code in `trainLinearReg` (found in the module `utils.py`) to compute the optimal values of $\\theta$. This training function uses `scipy`'s optimization module to minimize the cost function.\n",
    "\n",
    "In this part, we set regularization parameter $\\lambda$ to zero. Because our current implementation of linear regression is trying to fit a 2-dimensional $\\theta$, regularization will not be incredibly helpful for a $\\theta$ of such low dimension. In the later parts of the exercise, you will be using polynomial regression with regularization.\n",
    "\n",
    "Finally, the code in the next cell should also plot the best fit line, which should look like the figure below. \n",
    "\n",
    "![](Figures/linear_fit.png)\n",
    "\n",
    "The best fit line tells us that the model is not a good fit to the data because the data has a non-linear pattern. While visualizing the best fit as shown is one possible way to debug your learning algorithm, it is not always easy to visualize the data and model. In the next section, you will implement a function to generate learning curves that can help you debug your learning algorithm even if it is not easy to visualize the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a columns of ones for the y-intercept\n",
    "X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "theta = utils.trainLinearReg(linearRegCostFunction, X_aug, y, lambda_=0)\n",
    "\n",
    "#  Plot fit over the data\n",
    "pyplot.plot(X, y, 'ro', ms=10, mec='k', mew=1.5)\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.plot(X, np.dot(X_aug, theta), '--', lw=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## 2 Bias-variance\n",
    "\n",
    "An important concept in machine learning is the bias-variance tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data.\n",
    "\n",
    "In this part of the exercise, you will plot training and test errors on a learning curve to diagnose bias-variance problems.\n",
    "\n",
    "### 2.1 Learning Curves\n",
    "\n",
    "You will now implement code to generate the learning curves that will be useful in debugging learning algorithms. Recall that a learning curve plots training and cross validation error as a function of training set size. Your job is to fill in the function `learningCurve` in the next cell, so that it returns a vector of errors for the training set and cross validation set.\n",
    "\n",
    "To plot the learning curve, we need a training and cross validation set error for different training set sizes. To obtain different training set sizes, you should use different subsets of the original training set `X`. Specifically, for a training set size of $i$, you should use the first $i$ examples (i.e., `X[:i, :]`\n",
    "and `y[:i]`).\n",
    "\n",
    "You can use the `trainLinearReg` function (by calling `utils.trainLinearReg(...)`) to find the $\\theta$ parameters. Note that the `lambda_` is passed as a parameter to the `learningCurve` function.\n",
    "After learning the $\\theta$ parameters, you should compute the error on the training and cross validation sets. Recall that the training error for a dataset is defined as\n",
    "\n",
    "$$ J_{\\text{train}} = \\frac{1}{2m} \\left[ \\sum_{i=1}^m \\left(h_\\theta \\left( x^{(i)} \\right) - y^{(i)} \\right)^2 \\right] $$\n",
    "\n",
    "In particular, note that the training error does not include the regularization term. One way to compute the training error is to use your existing cost function and set $\\lambda$ to 0 only when using it to compute the training error and cross validation error. When you are computing the training set error, make sure you compute it on the training subset (i.e., `X[:n,:]` and `y[:n]`) instead of the entire training set. However, for the cross validation error, you should compute it over the entire cross validation set. You should store\n",
    "the computed errors in the vectors error train and error val.\n",
    "\n",
    "<a id=\"func2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, lambda_=0):\n",
    "    \"\"\"\n",
    "    Generates the train and cross validation set errors needed to plot a learning curve\n",
    "    returns the train and cross validation set errors for a learning curve. \n",
    "    \n",
    "    In this function, you will compute the train and test errors for\n",
    "    dataset sizes from 1 up to m. In practice, when working with larger\n",
    "    datasets, you might want to do this in larger intervals.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The training dataset. Matrix with shape (m x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each training datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    Xval : array_like\n",
    "        The validation dataset. Matrix with shape (m_val x n + 1) where m is the \n",
    "        total number of examples, and n is the number of features \n",
    "        before adding the bias term.\n",
    "    \n",
    "    yval : array_like\n",
    "        The functions values at each validation datapoint. A vector of\n",
    "        shape (m_val, ).\n",
    "    \n",
    "    lambda_ : float, optional\n",
    "        The regularization parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    error_train : array_like\n",
    "        A vector of shape m. error_train[i] contains the training error for\n",
    "        i examples.\n",
    "    error_val : array_like\n",
    "        A vecotr of shape m. error_val[i] contains the validation error for\n",
    "        i training examples.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return training errors in error_train and the\n",
    "    cross validation errors in error_val. i.e., error_train[i] and \n",
    "    error_val[i] should give you the errors obtained after training on i examples.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - You should evaluate the training error on the first i training\n",
    "      examples (i.e., X[:i, :] and y[:i]).\n",
    "    \n",
    "      For the cross-validation error, you should instead evaluate on\n",
    "      the _entire_ cross validation set (Xval and yval).\n",
    "    \n",
    "    - If you are using your cost function (linearRegCostFunction) to compute\n",
    "      the training and cross validation error, you should call the function with\n",
    "      the lambda argument set to 0. Do note that you will still need to use\n",
    "      lambda when running the training to obtain the theta parameters.\n",
    "    \n",
    "    Hint\n",
    "    ----\n",
    "    You can loop over the examples with the following:\n",
    "     \n",
    "           for i in range(1, m+1):\n",
    "               # Compute train/cross validation errors using training examples \n",
    "               # X[:i, :] and y[:i], storing the result in \n",
    "               # error_train[i-1] and error_val[i-1]\n",
    "               ....  \n",
    "    \"\"\"\n",
    "    # Number of training examples\n",
    "    m = y.size\n",
    "\n",
    "    # You need to return these values correctly\n",
    "    error_train = np.zeros(m)\n",
    "    error_val   = np.zeros(m)\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "         \n",
    "\n",
    "        \n",
    "    # =============================================================\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished implementing the function `learningCurve`, executing the next cell prints the learning curves and produce a plot similar to the figure below. \n",
    "\n",
    "![](Figures/learning_curve.png)\n",
    "\n",
    "In the learning curve figure, you can observe that both the train error and cross validation error are high when the number of training examples is increased. This reflects a high bias problem in the model - the linear regression model is too simple and is unable to fit our dataset well. In the next section, you will implement polynomial regression to fit a better model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aug = np.concatenate([np.ones((m, 1)), X], axis=1)\n",
    "Xval_aug = np.concatenate([np.ones((yval.size, 1)), Xval], axis=1)\n",
    "error_train, error_val = learningCurve(X_aug, y, Xval_aug, yval, lambda_=0)\n",
    "\n",
    "pyplot.plot(np.arange(1, m+1), error_train, np.arange(1, m+1), error_val, lw=2)\n",
    "pyplot.title('Learning curve for linear regression')\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 150])\n",
    "\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader[3] = learningCurve\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "\n",
    "## 3 Polynomial regression\n",
    "\n",
    "The problem with our linear model was that it was too simple for the data\n",
    "and resulted in underfitting (high bias). In this part of the exercise, you will address this problem by adding more features. For polynomial regression, our hypothesis has the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h_\\theta(x)  &= \\theta_0 + \\theta_1 \\times (\\text{waterLevel}) + \\theta_2 \\times (\\text{waterLevel})^2 + \\cdots + \\theta_p \\times (\\text{waterLevel})^p \\\\\n",
    "& = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots + \\theta_p x_p\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Notice that by defining $x_1 = (\\text{waterLevel})$, $x_2 = (\\text{waterLevel})^2$ , $\\cdots$, $x_p =\n",
    "(\\text{waterLevel})^p$, we obtain a linear regression model where the features are the various powers of the original value (waterLevel).\n",
    "\n",
    "Now, you will add more features using the higher powers of the existing feature $x$ in the dataset. Your task in this part is to complete the code in the function `polyFeatures` in the next cell. The function should map the original training set $X$ of size $m \\times 1$ into its higher powers. Specifically, when a training set $X$ of size $m \\times 1$ is passed into the function, the function should return a $m \\times p$ matrix `X_poly`, where column 1 holds the original values of X, column 2 holds the values of $X^2$, column 3 holds the values of $X^3$, and so on. Note that you don’t have to account for the zero-eth power in this function.\n",
    "\n",
    "<a id=\"polyFeatures\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, p):\n",
    "    \"\"\"\n",
    "    Maps X (1D vector) into the p-th power.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        A data vector of size m, where m is the number of examples.\n",
    "    \n",
    "    p : int\n",
    "        The polynomial power to map the features. \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    X_poly : array_like\n",
    "        A matrix of shape (m x p) where p is the polynomial \n",
    "        power and m is the number of examples. That is:\n",
    "    \n",
    "        X_poly[i, :] = [X[i], X[i]**2, X[i]**3 ...  X[i]**p]\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Given a vector X, return a matrix X_poly where the p-th column of\n",
    "    X contains the values of X to the p-th power.\n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly.\n",
    "    X_poly = np.zeros((X.shape[0], p))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a function that will map features to a higher dimension. The next cell will apply it to the training set, the test set, and the cross validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 8\n",
    "\n",
    "# Map X onto Polynomial Features and Normalize\n",
    "X_poly = polyFeatures(X, p)\n",
    "X_poly, mu, sigma = utils.featureNormalize(X_poly)\n",
    "X_poly = np.concatenate([np.ones((m, 1)), X_poly], axis=1)\n",
    "\n",
    "# Map X_poly_test and normalize (using mu and sigma)\n",
    "X_poly_test = polyFeatures(Xtest, p)\n",
    "X_poly_test -= mu\n",
    "X_poly_test /= sigma\n",
    "X_poly_test = np.concatenate([np.ones((ytest.size, 1)), X_poly_test], axis=1)\n",
    "\n",
    "# Map X_poly_val and normalize (using mu and sigma)\n",
    "X_poly_val = polyFeatures(Xval, p)\n",
    "X_poly_val -= mu\n",
    "X_poly_val /= sigma\n",
    "X_poly_val = np.concatenate([np.ones((yval.size, 1)), X_poly_val], axis=1)\n",
    "\n",
    "print('Normalized Training Example 1:')\n",
    "X_poly[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader[4] = polyFeatures\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Learning Polynomial Regression\n",
    "\n",
    "After you have completed the function `polyFeatures`, we will proceed to train polynomial regression using your linear regression cost function.\n",
    "\n",
    "Keep in mind that even though we have polynomial terms in our feature vector, we are still solving a linear regression optimization problem. The polynomial terms have simply turned into features that we can use for linear regression. We are using the same cost function and gradient that you wrote for the earlier part of this exercise.\n",
    "\n",
    "For this part of the exercise, you will be using a polynomial of degree 8. It turns out that if we run the training directly on the projected data, will not work well as the features would be badly scaled (e.g., an example with $x = 40$ will now have a feature $x_8 = 40^8 = 6.5 \\times 10^{12}$). Therefore, you will\n",
    "need to use feature normalization.\n",
    "\n",
    "Before learning the parameters $\\theta$ for the polynomial regression, we first call `featureNormalize` and normalize the features of the training set, storing the mu, sigma parameters separately. We have already implemented this function for you (in `utils.py` module) and it is the same function from the first exercise.\n",
    "\n",
    "After learning the parameters $\\theta$, you should see two plots generated for polynomial regression with $\\lambda = 0$, which should be similar to the ones here:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/polynomial_regression.png\"></td>\n",
    "        <td><img src=\"Figures/polynomial_learning_curve.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "You should see that the polynomial fit is able to follow the datapoints very well, thus, obtaining a low training error. The figure on the right shows that the training error essentially stays zero for all numbers of training samples. However, the polynomial fit is very complex and even drops off at the extremes. This is an indicator that the polynomial regression model is overfitting the training data and will not generalize well.\n",
    "\n",
    "To better understand the problems with the unregularized ($\\lambda = 0$) model, you can see that the learning curve  shows the same effect where the training error is low, but the cross validation error is high. There is a gap between the training and cross validation errors, indicating a high variance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0\n",
    "theta = utils.trainLinearReg(linearRegCostFunction, X_poly, y,\n",
    "                             lambda_=lambda_, maxiter=55)\n",
    "\n",
    "# Plot training data and fit\n",
    "pyplot.plot(X, y, 'ro', ms=10, mew=1.5, mec='k')\n",
    "\n",
    "utils.plotFit(polyFeatures, np.min(X), np.max(X), mu, sigma, theta, p)\n",
    "\n",
    "pyplot.xlabel('Change in water level (x)')\n",
    "pyplot.ylabel('Water flowing out of the dam (y)')\n",
    "pyplot.title('Polynomial Regression Fit (lambda = %f)' % lambda_)\n",
    "pyplot.ylim([-20, 50])\n",
    "\n",
    "pyplot.figure()\n",
    "error_train, error_val = learningCurve(X_poly, y, X_poly_val, yval, lambda_)\n",
    "pyplot.plot(np.arange(1, 1+m), error_train, np.arange(1, 1+m), error_val)\n",
    "\n",
    "pyplot.title('Polynomial Regression Learning Curve (lambda = %f)' % lambda_)\n",
    "pyplot.xlabel('Number of training examples')\n",
    "pyplot.ylabel('Error')\n",
    "pyplot.axis([0, 13, 0, 100])\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "\n",
    "print('Polynomial Regression (lambda = %f)\\n' % lambda_)\n",
    "print('# Training Examples\\tTrain Error\\tCross Validation Error')\n",
    "for i in range(m):\n",
    "    print('  \\t%d\\t\\t%f\\t%f' % (i+1, error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to combat the overfitting (high-variance) problem is to add regularization to the model. In the next section, you will get to  try different $\\lambda$ parameters to see how regularization can lead to a better model.\n",
    "\n",
    "### 3.2 Optional (ungraded) exercise: Adjusting the regularization parameter\n",
    "\n",
    "In this section, you will get to observe how the regularization parameter affects the bias-variance of regularized polynomial regression. You should now modify the the lambda parameter and try $\\lambda = 1, 100$. For each of these values, the script should generate a polynomial fit to the data and also a learning curve.\n",
    "\n",
    "For $\\lambda = 1$, the generated plots should look like the the figure below. You should see a polynomial fit that follows the data trend well (left) and a learning curve (right) showing that both the cross validation and training error converge to a relatively low value. This shows the $\\lambda = 1$ regularized polynomial regression model does not have the high-bias or high-variance problems. In effect, it achieves a good trade-off between bias and variance.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Figures/polynomial_regression_reg_1.png\"></td>\n",
    "        <td><img src=\"Figures/polynomial_learning_curve_reg_1.png\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "For $\\lambda = 100$, you should see a polynomial fit (figure below) that does not follow the data well. In this case, there is too much regularization and the model is unable to fit the training data.\n",
    "\n",
    "![](Figures/polynomial_regression_reg_100.png)\n",
    "\n",
    "*You do not need to submit any solutions for this optional (ungraded) exercise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "### 3.3 Selecting $\\lambda$ using a cross validation set\n",
    "\n",
    "From the previous parts of the exercise, you observed that the value of $\\lambda$ can significantly affect the results of regularized polynomial regression on the training and cross validation set. In particular, a model without regularization ($\\lambda = 0$) fits the training set well, but does not generalize. Conversely, a model with too much regularization ($\\lambda = 100$) does not fit the training set and testing set well. A good choice of $\\lambda$ (e.g., $\\lambda = 1$) can provide a good fit to the data.\n",
    "\n",
    "In this section, you will implement an automated method to select the $\\lambda$ parameter. Concretely, you will use a cross validation set to evaluate how good each $\\lambda$ value is. After selecting the best $\\lambda$ value using the cross validation set, we can then evaluate the model on the test set to estimate\n",
    "how well the model will perform on actual unseen data. \n",
    "\n",
    "Your task is to complete the code in the function `validationCurve`. Specifically, you should should use the `utils.trainLinearReg` function to train the model using different values of $\\lambda$ and compute the training error and cross validation error. You should try $\\lambda$ in the following range: {0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}.\n",
    "<a id=\"validationCurve\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationCurve(X, y, Xval, yval):\n",
    "    \"\"\"\n",
    "    Generate the train and validation errors needed to plot a validation\n",
    "    curve that we can use to select lambda_.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The training dataset. Matrix with shape (m x n) where m is the \n",
    "        total number of training examples, and n is the number of features \n",
    "        including any polynomial features.\n",
    "    \n",
    "    y : array_like\n",
    "        The functions values at each training datapoint. A vector of\n",
    "        shape (m, ).\n",
    "    \n",
    "    Xval : array_like\n",
    "        The validation dataset. Matrix with shape (m_val x n) where m is the \n",
    "        total number of validation examples, and n is the number of features \n",
    "        including any polynomial features.\n",
    "    \n",
    "    yval : array_like\n",
    "        The functions values at each validation datapoint. A vector of\n",
    "        shape (m_val, ).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lambda_vec : list\n",
    "        The values of the regularization parameters which were used in \n",
    "        cross validation.\n",
    "    \n",
    "    error_train : list\n",
    "        The training error computed at each value for the regularization\n",
    "        parameter.\n",
    "    \n",
    "    error_val : list\n",
    "        The validation error computed at each value for the regularization\n",
    "        parameter.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Fill in this function to return training errors in `error_train` and\n",
    "    the validation errors in `error_val`. The vector `lambda_vec` contains\n",
    "    the different lambda parameters to use for each calculation of the\n",
    "    errors, i.e, `error_train[i]`, and `error_val[i]` should give you the\n",
    "    errors obtained after training with `lambda_ = lambda_vec[i]`.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    You can loop over lambda_vec with the following:\n",
    "    \n",
    "          for i in range(len(lambda_vec))\n",
    "              lambda = lambda_vec[i]\n",
    "              # Compute train / val errors when training linear \n",
    "              # regression with regularization parameter lambda_\n",
    "              # You should store the result in error_train[i]\n",
    "              # and error_val[i]\n",
    "              ....\n",
    "    \"\"\"\n",
    "    # Selected values of lambda (you should not change this)\n",
    "    lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "    # You need to return these variables correctly.\n",
    "    error_train = np.zeros(len(lambda_vec))\n",
    "    error_val = np.zeros(len(lambda_vec))\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return lambda_vec, error_train, error_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have completed the code, the next cell will run your function and plot a cross validation curve of error v.s. $\\lambda$ that allows you select which $\\lambda$ parameter to use. You should see a plot similar to the figure below. \n",
    "\n",
    "![](Figures/cross_validation.png)\n",
    "\n",
    "In this figure, we can see that the best value of $\\lambda$ is around 3. Due to randomness\n",
    "in the training and validation splits of the dataset, the cross validation error can sometimes be lower than the training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec, error_train, error_val = validationCurve(X_poly, y, X_poly_val, yval)\n",
    "\n",
    "pyplot.plot(lambda_vec, error_train, '-o', lambda_vec, error_val, '-o', lw=2)\n",
    "pyplot.legend(['Train', 'Cross Validation'])\n",
    "pyplot.xlabel('lambda')\n",
    "pyplot.ylabel('Error')\n",
    "\n",
    "print('lambda\\t\\tTrain Error\\tValidation Error')\n",
    "for i in range(len(lambda_vec)):\n",
    "    print(' %f\\t%f\\t%f' % (lambda_vec[i], error_train[i], error_val[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*You should now submit your solutions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader[5] = validationCurve\n",
    "grader.grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4  Optional (ungraded) exercise: Computing test set error\n",
    "\n",
    "In the previous part of the exercise, you implemented code to compute the cross validation error for various values of the regularization parameter $\\lambda$. However, to get a better indication of the model’s performance in the real world, it is important to evaluate the “final” model on a test set that was not used in any part of training (that is, it was neither used to select the $\\lambda$ parameters, nor to learn the model parameters $\\theta$). For this optional (ungraded) exercise, you should compute the test error using the best value of $\\lambda$ you found. In our cross validation, we obtained a test error of 3.8599 for $\\lambda = 3$.\n",
    "\n",
    "*You do not need to submit any solutions for this optional (ungraded) exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Optional (ungraded) exercise: Plotting learning curves with randomly selected examples\n",
    "\n",
    "In practice, especially for small training sets, when you plot learning curves to debug your algorithms, it is often helpful to average across multiple sets of randomly selected examples to determine the training error and cross validation error.\n",
    "\n",
    "Concretely, to determine the training error and cross validation error for $i$ examples, you should first randomly select $i$ examples from the training set and $i$ examples from the cross validation set. You will then learn the parameters $\\theta$ using the randomly chosen training set and evaluate the parameters $\\theta$ on the randomly chosen training set and cross validation set. The above steps should then be repeated multiple times (say 50) and the averaged error should be used to determine the training error and cross validation error for $i$ examples.\n",
    "\n",
    "For this optional (ungraded) exercise, you should implement the above strategy for computing the learning curves. For reference, the figure below  shows the learning curve we obtained for polynomial regression with $\\lambda = 0.01$. Your figure may differ slightly due to the random selection of examples.\n",
    "\n",
    "![](Figures/learning_curve_random.png)\n",
    "\n",
    "*You do not need to submit any solutions for this optional (ungraded) exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}